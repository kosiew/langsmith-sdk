{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79036a8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tracing without LangChain\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langsmith-cookbook/blob/main/tracing-examples/traceable/tracing_without_langchain.ipynb)\n",
    "\n",
    "LangSmith lets you instrument **any LLM application,** no LangChain required. This aids in debugging, evaluating, and monitoring your app, without needing to learn any particular framework's unique semantics.\n",
    "\n",
    "LangSmith instruments your apps through run traces. Each trace is made of 1 or more \"runs\" representing key event spans in your app. Each run is a structured log with a name, `run_type`, inputs / outputs, start/end times, as well as tags and other metadata. Runs within a trace are nested, forming a hierarchy referred to as a \"run tree.\"\n",
    "\n",
    "In Python, LangSmith offers two ways to help manage run trees:\n",
    "\n",
    "1. `RunTree` object: manages run data and communicates with the LangSmith platform. The `create_child` method simplifies the creation and organization of child runs within the hierarchy.\n",
    "2. `@traceable` decorator: automates the process of instrumenting function calls, handling the RunTree lifecycle for you.\n",
    "\n",
    "This walkthrough guides you through creating a chat app and instrumenting it with the `@traceable` decorator. You will add tags and other metadata to your runs to help filter and organize the resulting traces. The chat app uses three 'chain' components to generate a text argument on a provided subject:\n",
    "\n",
    "1. An argument generation function\n",
    "2. Critique function\n",
    "3. Refine function\n",
    "\n",
    "Each of these functions will call an openai `llm` function, and the whole series of calls will itself be organized beneath a parent `argument_chain` function, generating a trace like this:\n",
    "\n",
    "![RunTree 1](img/snapshot_1.png)\n",
    "\n",
    "Ready to build? Let's start!\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Let's first install the required packages. The app uses `openai`'s SDK, and our tracing will use the `langsmith` SDK.\n",
    "\n",
    "Install the latest versions to make sure you have all the required updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5615479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langsmith > /dev/null\n",
    "# %pip install -U openai > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ebe2c0",
   "metadata": {},
   "source": [
    "Next, configure the API Key in the environment to make sure traces are logged to your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f665dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"  # Update with your API key\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "project_name = \"YOUR PROJECT NAME\"  # Update with your project name\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = project_name  # Optional: \"default\" is used if not set\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI API KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90649d87-bf4c-4882-a067-b666f37a88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_name = \"pass-parent\"  # Update with your project name\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = project_name  # Optional: \"default\" is used if not set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17285047",
   "metadata": {},
   "source": [
    "## Using the decorator\n",
    "\n",
    "Next, define your chat application. Use the `@traceable` decorator to automatically instrument your\n",
    "function calls.\n",
    "\n",
    "The decorator works by creating a run tree for you each time the function is called and inserting it within the current trace. The function inputs, name, and other information is then streamed to LangSmith. If the function raises an error or if it returns a response, that information is also added to the tree, and updates are patched to LangSmith so you can detect and diagnose sources of errors. This is all done on a background thread to avoid blocking your app's execution.\n",
    "\n",
    "The app below combines three runs with the \"chain\" `run_type`. Run types can be thought of as a special type of tags to help index and visualize the logged information. Each of our \"chain\" runs call an function typed as an \"llm\" run. These correspond LLM or chat model calls. We recommend you type most functions as `chain` runs, as these are the most general-purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3bb18d-b331-48db-a39b-3000eaff1079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from langsmith import run_helpers\n",
    "\n",
    "# If you've made changes to traceable and want to reload it:\n",
    "importlib.reload(run_helpers)\n",
    "\n",
    "# Now you can use the updated traceable function\n",
    "from langsmith.run_helpers import traceable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba8c359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "from openai import OpenAI\n",
    "from langsmith.run_helpers import traceable\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "# We will label this function as an 'llm' call to better organize it\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_openai(\n",
    "    messages: List[dict], model: str = \"gpt-4o\", temperature: float = 0.0\n",
    ") -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "\n",
    "# The 'chain' run_type can correspond to any function and is the most common\n",
    "@traceable(run_type=\"chain\")\n",
    "def argument_generator(query: str, additional_description: str = \"\") -> str:\n",
    "    return (\n",
    "        call_openai(\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"You are a debater making an argument on a topic.\"\n",
    "                    f\"{additional_description}\"\n",
    "                    f\" The current time is {datetime.now()}\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": f\"The discussion topic is {query}\"},\n",
    "            ]\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def critic(argument: str) -> str:\n",
    "    return (\n",
    "        call_openai(\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"You are a critic.\"\n",
    "                    \"\\nWhat unresolved questions or criticism do you have after reading the following argument?\"\n",
    "                    \"Provide a concise summary of your feedback.\",\n",
    "                },\n",
    "                {\"role\": \"system\", \"content\": argument},\n",
    "            ]\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def refiner(\n",
    "    query: str, additional_description: str, current_arg: str, criticism: str\n",
    ") -> str:\n",
    "    return (\n",
    "        call_openai(\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"You are a debater making an argument on a topic.\"\n",
    "                    f\"{additional_description}\"\n",
    "                    f\" The current time is {datetime.now()}\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": f\"The discussion topic is {query}\"},\n",
    "                {\"role\": \"assistant\", \"content\": current_arg},\n",
    "                {\"role\": \"user\", \"content\": criticism},\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Please generate a new argument that incorporates the feedback from the user.\",\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def argument_chain(query: str, additional_description: str = \"\") -> str:\n",
    "    argument = argument_generator(query, additional_description)\n",
    "    criticism = critic(argument)\n",
    "    return refiner(query, additional_description, argument, criticism)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fb9f73",
   "metadata": {},
   "source": [
    "Now call the chain. If you set up your API key correctly at the start of this notebook, all the results should be traced to [LangSmith](https://smith.langchain.com). We will prompt the app to generate an argument that sunshine is good for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f5798a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunshine offers numerous health benefits, including the production of vitamin D, which is essential for bone health, immune function, and mood regulation. Natural light also helps regulate circadian rhythms, leading to better sleep and overall well-being. However, it is important to acknowledge the potential risks of overexposure, such as skin cancer and other skin-related issues. Individual needs for sunlight can vary based on factors like skin type, geographic location, and existing health conditions. Therefore, while moderate sun exposure is beneficial, it is crucial to balance it with protective measures like sunscreen and appropriate clothing to maximize health benefits while minimizing risks.\n"
     ]
    }
   ],
   "source": [
    "result = argument_chain(\n",
    "    \"Whether sunshine is good for you.\",\n",
    "    additional_description=\"Provide a concise, few sentence argument on why sunshine is good for you.\",\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad8af8",
   "metadata": {},
   "source": [
    "## Working with runs\n",
    "\n",
    "The above is all you need to save your app traces to LangSmith! You can try changing the functions or even raising errors in the above code to see how it's visualized in [LangSmith](https://smith.langchain.com).\n",
    "\n",
    "The decorator does all this work behind the scenes, but you may want to be able to actually use the run ID for other things like monitoring user feedback or logging to another destination. You can easily do this by modifying your function signature to accept a `run_tree` keyword argument. Doing so will instuct the `@traceable` decorator to inject the current run tree object into your function. This can be useful if you want to:\n",
    "- Add user feedback to the run\n",
    "- Inspect or save the run ID or its parent\n",
    "- Manually log child runs or their information to another destination\n",
    "- Continue a trace in other functions that may be called within a separate thread or process pool (or even on separate machines)\n",
    "\n",
    "Below, our `argument_chain2` function is identical to the previous one except that we return the ID of the `run_tree` for use outside the function context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117dca62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from uuid import UUID\n",
    "\n",
    "from langsmith import RunTree\n",
    "\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def argument_chain2(\n",
    "    query: str, *, additional_description: str = \"\", run_tree: RunTree\n",
    ") -> Tuple[str, UUID]:\n",
    "    argument = argument_generator(query, additional_description)\n",
    "    criticism = critic(argument)\n",
    "    refined_argument = refiner(query, additional_description, argument, criticism)\n",
    "    return refined_argument, run_tree.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b339dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We don't need to pass in the `run_tree` when calling the function. It will be injected\n",
    "# by the decorator\n",
    "result, run_id = argument_chain2(\n",
    "    \"Whether sunshine is good for you.\",\n",
    "    additional_description=\"Provide a concise, few sentence argument on why sunshine is good for you.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c8ea3",
   "metadata": {},
   "source": [
    "With the run ID, you can do things like log feedback from a user after the run is completed, or you can create a public link to share. We will do both below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e45a0845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf7b378c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('37834773-7019-4fb3-a790-5816e8180738'), created_at=datetime.datetime(2024, 6, 11, 5, 38, 45, 225144, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 11, 5, 38, 45, 225151, tzinfo=datetime.timezone.utc), run_id=UUID('34fe114f-f683-4580-be5a-3efb827ccca1'), key='user_feedback', score=0.5, value=None, comment=None, correction={'generation': 'Sunshine is nice. Full stop.'}, feedback_source=FeedbackSourceBase(type='api', metadata={}), session_id=None, comparative_experiment_id=None, feedback_group_id=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_feedback(\n",
    "    run_id,\n",
    "    \"user_feedback\",\n",
    "    score=0.5,\n",
    "    correction={\"generation\": \"Sunshine is nice. Full stop.\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1656dd86",
   "metadata": {},
   "source": [
    "[![RunTree 2](./img/snapshot_2.png)](https://smith.langchain.com/public/7e7b6caa-80bc-41af-bed1-35cd1011de77/r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc2fad",
   "metadata": {},
   "source": [
    "## Configuring traces\n",
    "\n",
    "One way to make your application traces more useful or actionable is to tag or add metadata to the logs. That way you can do things like track the version of your code or deployment environment in a single project.\n",
    "\n",
    "The traceable decorator can be configured to add additional information such as:\n",
    "- string tags\n",
    "- arbitrary key-value metadata\n",
    "- custom trace names\n",
    "- manually-specified run ID\n",
    "\n",
    "Below is an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac98115b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can add tags and metadata (or even the project name) directly in the decorator\n",
    "@traceable(\n",
    "    run_type=\"chain\",\n",
    "    name=\"My Argument Chain\",\n",
    "    tags=[\"tutorial\"],\n",
    "    metadata={\"githash\": \"e38f04c83\"},\n",
    ")\n",
    "def argument_chain_3(query: str, additional_description: str = \"\") -> str:\n",
    "    argument = argument_generator(query, additional_description)\n",
    "    criticism = critic(argument)\n",
    "    return refiner(query, additional_description, argument, criticism)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e54d9-5107-4f93-a76e-52dd2f9d2f96",
   "metadata": {},
   "source": [
    "This information can also be added when the function is called. This is done by passing a dictionary to the `langsmith_extra` argument. \n",
    "\n",
    "Below, let's manually add some tags and the UUID for the run. We can then use this UUID in the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "217e03ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "# You can manually specify an ID rather than letting\n",
    "# the decorator generate one for you\n",
    "requested_uuid = uuid4()\n",
    "\n",
    "result = argument_chain_3(\n",
    "    \"Whether sunshine is good for you.\",\n",
    "    additional_description=\"Provide a concise, few sentence argument on why sunshine is good for you.\",\n",
    "    # You can also add tags, metadata, or the run ID directly via arguments to the langsmith_extra argument\n",
    "    # at all-time.\n",
    "    langsmith_extra={\n",
    "        \"tags\": [\"another-tag\"],\n",
    "        \"metadata\": {\"another-key\": 1},\n",
    "        \"run_id\": requested_uuid,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de84533a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('61ae8bdd-3175-4a79-89ca-da86639de57c'), created_at=datetime.datetime(2024, 6, 11, 5, 43, 58, 198753, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 11, 5, 43, 58, 198754, tzinfo=datetime.timezone.utc), run_id=UUID('b3da8354-c00d-46b9-8f57-09337d928888'), key='user_feedback', score=1, value=None, comment=None, correction=None, feedback_source=FeedbackSourceBase(type='api', metadata={'origin': 'example notebook'}), session_id=None, comparative_experiment_id=None, feedback_group_id=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can log feedback for the run directly since we've controlled the ID it's assuming\n",
    "client.create_feedback(\n",
    "    requested_uuid, \"user_feedback\", score=1, source_info={\"origin\": \"example notebook\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b955cab",
   "metadata": {},
   "source": [
    "Now you can navigate to the run with the requested UUID to see the simulated \"user feedback\".\n",
    "\n",
    "[![Tagged Run Tree](./img/snapshot_3.png)](https://dev.smith.langchain.com/public/502a9bc4-44b1-4e86-bb19-3d6b9766f248/r)\n",
    "\n",
    "Clicking in to the 'Metadata' tab, you can see the metadata has been stored for the trace.\n",
    "\n",
    "[![Run Tree Metadata](./img/snapshot_3_metadata.png)](https://dev.smith.langchain.com/public/502a9bc4-44b1-4e86-bb19-3d6b9766f248/r?tab=2)\n",
    "\n",
    "Once you've stored these tagged runs, you can filter and search right in the web app by clicking on the suggested filters or by writing out the query in the search bar:\n",
    "\n",
    "\n",
    "![Filtering Tags](./img/snapshot_3_tag_filter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212bffc-9798-43af-8dd7-d3c5fbf72582",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "In this walkthrough, you made an example LLM application and instrumented it using the `@traceable` decorator from the LangSmith SDK.\n",
    "\n",
    "You also added tags and metadata and even logged feedback to the runs. The traceable decorator is a light-weight and flexible way to start debugging and monitoring your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ba84e10-9898-430b-bec5-f9ad535aeab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable, get_current_run_tree\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "@traceable\n",
    "def outer_func():\n",
    "    rt = get_current_run_tree()\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        r = list(\n",
    "            executor.map(\n",
    "                lambda x: inner_func(x, langsmith_extra={\"parent\": rt}), [1, 2]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "@traceable\n",
    "def inner_func(x):\n",
    "    print(x)\n",
    "\n",
    "\n",
    "outer_func()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aabb89c9-5ed1-49d0-851b-407e17a544c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PydanticOutputParser\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic_v1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field, validator\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d8095-315f-4bb3-828b-051872529725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
